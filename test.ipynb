{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, MiniBatchKMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics.cluster import adjusted_rand_score, adjusted_mutual_info_score\n",
    "from numpy.random import random_sample\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import sklearn\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['file_name', 'attr_conc.mean', 'attr_conc.sd', 'attr_ent.mean',\n",
       "       'attr_ent.sd', 'attr_to_inst', 'cohesiveness.mean', 'cohesiveness.sd',\n",
       "       'cor.mean', 'cor.sd', 'cov.mean', 'cov.sd', 'eigenvalues.mean',\n",
       "       'eigenvalues.sd', 'inst_to_attr', 'iq_range.mean', 'iq_range.sd',\n",
       "       'kurtosis.mean', 'kurtosis.sd', 'mad.mean', 'mad.sd', 'max.mean',\n",
       "       'max.sd', 'mean.mean', 'mean.sd', 'median.mean', 'median.sd',\n",
       "       'min.mean', 'min.sd', 'nr_attr', 'nr_cor_attr', 'nr_inst',\n",
       "       'one_itemset.mean', 'one_itemset.sd', 'range.mean', 'range.sd',\n",
       "       'sd.mean', 'sd.sd', 'skewness.mean', 'skewness.sd', 'sparsity.mean',\n",
       "       'sparsity.sd', 't2', 't3', 't4', 't_mean.mean', 't_mean.sd',\n",
       "       'two_itemset.mean', 'two_itemset.sd', 'var.mean', 'var.sd',\n",
       "       'wg_dist.mean', 'wg_dist.sd'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Surrogate_Sv5.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1DxToJ3EEWQokxhO20e5k2OpR-_Nr5l_R\n",
    "\n",
    "#Importando base e criando variÃ¡veis\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import wandb\n",
    "import sys\n",
    "\n",
    "### WANDB\n",
    "api = wandb.Api()\n",
    "###\n",
    "\n",
    "#PATH = \"/home/barbon/PycharmProjects/Surrogate_SV5/\"\n",
    "PATH = sys.argv[1]\n",
    "VERSION = \"Sv5_b\"\n",
    "FILE = \"metadatabase_surrogate_\"+VERSION+\".csv\"\n",
    "LOG_FILE = \"log_13_07_b.csv\"\n",
    "\n",
    "###################\n",
    "\n",
    "par_ini = sys.argv[2]\n",
    "par_fin = sys.argv[3]\n",
    "\n",
    "\n",
    "sil_ini_list = np.arange(0.2, 0.6, 0.05)\n",
    "dbs_max_list = np.arange(0.4, 0.8, 0.1)\n",
    "dist_sil_dbs_list = np.arange(0.15, 0.35, 0.05)\n",
    "\n",
    "seed_list = range(int(par_ini),int(par_fin))\n",
    "distribution_list = [[\"exponential\", 'gumbel', 'normal'], [\"exponential\", 'normal'], ['gumbel', 'normal']]\n",
    "#distribution_list = [[\"exponential\", 'normal']]\n",
    "algorithms = [RandomForestRegressor]\n",
    "qtd_arvores = [100,250]\n",
    "threshold_qtd_on_training = 50\n",
    "contamination = [0, 0.05, 0.1, 0.15]\n",
    "###################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"#Visualizando dados\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def datasets_statistics(meta_dataset):\n",
    "  #meta_dataset = pd.read_csv(PATH+FILE)\n",
    "  print(\"Original\", meta_dataset.shape)\n",
    "\n",
    "  #meta_dataset = meta_dataset.sample(frac=1, random_state=1, replace=True).reset_index(drop=True)\n",
    "  #meta_dataset = meta_dataset.drop_duplicates(subset=[\"Dataset\"], keep=\"last\")\n",
    "  #print(\"Final\", meta_dataset.shape)\n",
    "\n",
    "  meta_dataset['clusters'] = meta_dataset.apply(lambda row: int(row.Dataset.split('-')[1].replace(\"clusters\",\"\")), axis=1)\n",
    "  meta_dataset['aspectref'] = meta_dataset.apply(lambda row: float(row.Dataset.split('-')[7].replace(\"aspectref\",\"\")), axis=1)\n",
    "  meta_dataset['aspectmaxmin'] = meta_dataset.apply(lambda row: float(row.Dataset.split('-')[8].replace(\"aspectmaxmin\",\"\")), axis=1)\n",
    "  meta_dataset['radius'] = meta_dataset.apply(lambda row: float(row.Dataset.split('-')[9].replace(\"radius\",\"\")), axis=1)\n",
    "  meta_dataset['imbalance'] = meta_dataset.apply(lambda row: float(row.Dataset.split('-')[10].replace(\"imbalance\",\"\")), axis=1)\n",
    "  meta_dataset['distribution'] = meta_dataset.apply(lambda row: row.Dataset.split('\\'')[1], axis=1)\n",
    "\n",
    "  plt.figure()\n",
    "  meta_dataset[\"cluster_diff\"].hist(bins=len(meta_dataset[\"cluster_diff\"].unique()))\n",
    "  plt.figure()\n",
    "  meta_dataset[\"clusters\"].hist()\n",
    "  plt.figure()\n",
    "  meta_dataset[\"algorithm\"].hist()\n",
    "  plt.figure()\n",
    "  meta_dataset[\"distribution\"].hist()\n",
    "  plt.figure()\n",
    "  meta_dataset[\"aspectref\"].hist()\n",
    "  plt.figure()\n",
    "  meta_dataset[\"sil\"].hist(color='#FFCF56')\n",
    "  plt.figure()\n",
    "  meta_dataset[\"dbs\"].hist(color='#FF0000')\n",
    "\n",
    "\"\"\"#Surrogate Model\n",
    "\n",
    "### methods\n",
    "\"\"\"\n",
    "\n",
    "def filtering_distribution(database_input, dist):\n",
    "  database_input['distribution'] = database_input.apply(lambda row: row.Dataset.split('\\'')[1], axis=1)\n",
    "  return database_input[database_input.distribution.isin(dist)].drop([\"distribution\"], axis=1)\n",
    "\n",
    "def filter_sil_bds(database_input, min_sil, max_dbs):\n",
    "  database_input = database_input[database_input.sil>=min_sil]\n",
    "  database_input = database_input[database_input.dbs<=max_dbs]\n",
    "  return database_input\n",
    "\n",
    "def filter_dist_sil_bds(database_input, dist):\n",
    "  database_input = database_input[(database_input.dbs-database_input.sil) <= dist]\n",
    "  return database_input\n",
    "\n",
    "def filter_samples_isolation(database_input, contamination):\n",
    "    if contamination>0:\n",
    "        iso = IsolationForest(contamination=contamination, random_state=39)\n",
    "        yhat = iso.fit_predict(database_input._get_numeric_data())\n",
    "\n",
    "        mask = yhat != -1\n",
    "        database_input = database_input.iloc[mask, :]\n",
    "    return database_input\n",
    "\n",
    "def plot_train(model):\n",
    "  results = model.evals_result()\n",
    "\n",
    "  plt.figure(figsize=(10,7))\n",
    "  plt.plot(results[\"validation_0\"][\"rmse\"], label=\"Training loss\")\n",
    "  plt.plot(results[\"validation_1\"][\"rmse\"], label=\"Validation loss\")\n",
    "  plt.axvline(model.best_ntree_limit, color=\"gray\", label=\"Optimal tree number\")\n",
    "  plt.xlabel(\"Number of trees\")\n",
    "  plt.ylabel(\"Loss\")\n",
    "  plt.legend()\n",
    "\n",
    "def model_prediction_analysis(model_regressor, dataset_test, dataset_y_test):\n",
    "  print(dataset_test.shape)\n",
    "  yhat = model_regressor.predict(dataset_test)\n",
    "\n",
    "  mae = metrics.mean_absolute_error(dataset_y_test, yhat)\n",
    "  print('MAE', mae)\n",
    "  mse = metrics.mean_squared_error(dataset_y_test, yhat)\n",
    "  print('MSE', mse)\n",
    "  rmse = np.sqrt(mse) # or mse**(0.5)\n",
    "  print('RMSE', rmse)\n",
    "  print('Obtained:',yhat[0:10])\n",
    "  r2 = metrics.r2_score(dataset_y_test,yhat)\n",
    "  print('R2', r2)\n",
    "\n",
    "def plot_optimization(x, best_k):\n",
    "  best_k = best_k.reset_index()\n",
    "  for i in x.Dataset.unique():\n",
    "    plt.figure()\n",
    "    sns.lineplot(data=x[x.Dataset == i], x=\"k_candidate\", y=\"yhat\")\n",
    "    sns.lineplot(data=x[x.Dataset == i], x=\"k_candidate\", y=\"sil\")\n",
    "    ax = sns.lineplot(data=x[x.Dataset == i], x=\"k_candidate\", y=\"dbs\")\n",
    "    ax.axvline(x = x[x.Dataset == i].k_expected.unique(), ymin = 0, ymax = 1)\n",
    "    ax.axvline(x = best_k[best_k.Dataset_x == i].k_candidate.unique(), ymin = 0, ymax = 1, c='r', linestyle='--')\n",
    "    ax.set_title(i)\n",
    "\n",
    "def minimizing(model_regressor, features, plot, log, seed):\n",
    "  datasets = run_exp(model_regressor, features, datasets_selected_benchmarking)\n",
    "  x = pd.DataFrame(datasets)\n",
    "  x.columns = [\"Dataset\", \"Algorithm\", \"sil\", \"dbs\", \"k_candidate\", \"k_expected\", \"yhat\"]\n",
    "  x['yhat'] = x.apply(lambda row: row.yhat[0], axis=1)\n",
    "  x\n",
    "\n",
    "  min_data = pd.DataFrame(x.groupby([\"Dataset\"])[\"yhat\"].min()).reset_index()\n",
    "  result = pd.merge(x, min_data, on=\"yhat\")\n",
    "  result\n",
    "\n",
    "  mae = np.round(metrics.mean_absolute_error(result.k_candidate, result.k_expected),2)\n",
    "  print(\"MAE\",mae)\n",
    "  mse = np.round(metrics.mean_squared_error(result.k_candidate, result.k_expected),2)\n",
    "  print(\"MSE\",mse)\n",
    "  rmse = np.round(np.sqrt(mse),2) # or mse**(0.5)\n",
    "  print(\"RMSE\",rmse)\n",
    "  r2 = np.round(metrics.r2_score(result.k_candidate,result.k_expected),2)\n",
    "  print(\"R2\",r2)\n",
    "  acc = np.round(metrics.accuracy_score(result.k_candidate,result.k_expected),2)\n",
    "  print(\"ACC\",acc)\n",
    "\n",
    "  print(\"Std Deviation\", result.yhat.std())\n",
    "\n",
    "  #print(result.groupby(['Dataset_x', 'Algorithm'])['k_candidate', 'k_expected'].mean())\n",
    "  print(result.groupby(['Dataset_x'])['k_candidate', 'k_expected'].min())\n",
    "\n",
    "  if plot:\n",
    "    plot_optimization(x, result.groupby(['Dataset_x'])['k_candidate', 'k_expected'].mean())\n",
    "\n",
    "  if log:\n",
    "\n",
    "    log = [mae, mse, rmse, r2, acc, seed, type(model_regressor).__name__]\n",
    "    log = pd.DataFrame(log).T\n",
    "    log.columns = [\"MAE\", \"MSE\", \"RMSE\", \"R2\", \"ACC\", \"seed\", \"algorithm\"]\n",
    "    log.to_csv(PATH+\"log_05_07_2023.csv\", mode='a', index=False, header=False)\n",
    "\n",
    "def minimizing_logging(model_regressor, features, sil_ini, dbs_max, dist_s_d, seed, dist, qtd, run, progress, contamin, qtd_arvores):\n",
    "  datasets = run_exp(model_regressor, features, datasets_selected_benchmarking)\n",
    "  x = pd.DataFrame(datasets)\n",
    "  x.columns = [\"Dataset\", \"Algorithm\", \"sil\", \"dbs\", \"k_candidate\", \"k_expected\", \"yhat\"]\n",
    "  x['yhat'] = x.apply(lambda row: row.yhat[0], axis=1)\n",
    "  x\n",
    "\n",
    "  min_data = pd.DataFrame(x.groupby([\"Dataset\"])[\"yhat\"].min().reset_index())\n",
    "  result = pd.merge(x, min_data, on=\"yhat\").drop_duplicates(subset='Dataset_x', keep=\"first\")\n",
    "\n",
    "  #print(result)\n",
    "\n",
    "  mae = np.round(metrics.mean_absolute_error(result.k_candidate, result.k_expected),2)\n",
    "  print(\"MAE\",mae)\n",
    "  mse = np.round(metrics.mean_squared_error(result.k_candidate, result.k_expected),2)\n",
    "  print(\"MSE\",mse)\n",
    "  rmse = np.round(np.sqrt(mse),2) # or mse**(0.5)\n",
    "  print(\"RMSE\",rmse)\n",
    "  r2 = np.round(metrics.r2_score(result.k_candidate,result.k_expected),2)\n",
    "  print(\"R2\",r2)\n",
    "  acc = np.round(metrics.accuracy_score(result.k_candidate,result.k_expected),2)\n",
    "  print(\"ACC\",acc)\n",
    "\n",
    "  std =  np.round(result.yhat.std(),2)\n",
    "  print(\"Std Deviation\", std)\n",
    "\n",
    "  contamination = np.round(contamin,2)\n",
    "\n",
    "  #print(result.groupby(['Dataset_x', 'Algorithm'])['k_candidate', 'k_expected'].mean())\n",
    "  #print(result.groupby(['Dataset_x'])['k_candidate', 'k_expected'].min().reset_index())\n",
    "  print(result.groupby(['Dataset_x'])[['k_candidate', 'k_expected']]\n",
    "        .mean())\n",
    "\n",
    "  log = [mae, mse, rmse, r2, acc, std, type(model_regressor).__name__, seed, sil_ini, dbs_max, dist_s_d, dist, qtd, contamination, qtd_arvores]\n",
    "  log = pd.DataFrame(log).T\n",
    "  log.columns = [\"MAE\", \"MSE\", \"RMSE\", \"R2\", \"ACC\",  \"STD all clusterers\", \"algorithm\", \"seed\",  \"sil_ini\", \"dbs_max\", \"distance_s_d\", \"distribution\", \"qtd samples\", \"contamination\", \"qtd_arvores\"]\n",
    "  log.to_csv(PATH+LOG_FILE, mode='a', index=False, header=False)\n",
    "\n",
    "  run.log({\"model_regressor\": \"RandomForest\",\n",
    "             \"features\": features,\n",
    "             \"sil_ini\": sil_ini,  # time cost\n",
    "             \"dbs_max\": dbs_max,  # parsimonly?\n",
    "             \"dist_s_d\": dist_s_d,\n",
    "             \"seed\": seed,\n",
    "             \"Distribution\": dist,\n",
    "             \"Qtd Samples\": qtd,\n",
    "             \"MAE\": mae,\n",
    "             \"MSE\": mse,\n",
    "             \"RMSE\": rmse,\n",
    "             \"R2\": r2,\n",
    "             \"ACC\": acc,\n",
    "             \"STD all clusterers\": std,\n",
    "             \"Progress\":progress,\n",
    "           \"Contamination\": contamination,\n",
    "           \"qtd_arvores\":qtd_arvores\n",
    "             })\n",
    "\n",
    "\"\"\"## benchmarking datasets and features\"\"\"\n",
    "\n",
    "datasets_selected_benchmarking = [\n",
    "                    'a1.csv',\n",
    "                    'iris.csv',\n",
    "                    'dim1024.csv', 'dim4.csv', 'dim5.csv', 'dim15.csv',\n",
    "                    'dim7.csv', 'dim6.csv',# 'dim8.csv', 'dim9.csv',\n",
    "                    #'dim512.csv', 'dim13.csv', 'dim10.csv', 'dim032.csv', 'dim12.csv', 'dim11.csv', 'dim14.csv',\n",
    "                    #'dim064.csv', 'dim256.csv',\n",
    "                    'dim128.csv', 'dim3.csv',\n",
    "                    'chainlink.csv',\n",
    "                    'cancer.csv',\n",
    "                    #'a2.csv',\n",
    "                    #'a3.csv',\n",
    "                    'atom.csv', 'lsun3d.csv', 'hepta.csv', 'engytime.csv',\n",
    "                    'target.csv', 'tetra.csv', 'twodiamonds.csv'#, 'wingnut.csv'\n",
    "                    ,'unbalance.csv'\n",
    "                    ]\n",
    "\n",
    "features = ['attr_conc.mean','attr_conc.sd','attr_ent.mean',\n",
    "            'attr_ent.sd','attr_to_inst','cohesiveness.mean','cohesiveness.sd',\n",
    "            'cor.mean',#'cor.sd',\n",
    "            'cov.mean',#'cov.sd',\n",
    "            'eigenvalues.mean','eigenvalues.sd',\n",
    "            'inst_to_attr','iq_range.mean','iq_range.sd','kurtosis.mean','kurtosis.sd',\n",
    "            'mad.mean','mad.sd',\n",
    "            #'max.mean','max.sd','mean.mean','mean.sd',\n",
    "            'median.mean',\n",
    "            'median.sd','min.mean','min.sd','nr_attr','nr_cor_attr','nr_inst','one_itemset.mean',\n",
    "            'one_itemset.sd',\n",
    "            #'range.mean','range.sd',\n",
    "            'sd.mean','sd.sd','skewness.mean',\n",
    "            'skewness.sd','sparsity.mean','sparsity.sd','t2','t3','t4','t_mean.mean',\n",
    "            't_mean.sd','two_itemset.mean','two_itemset.sd','var.mean','var.sd',\n",
    "            'wg_dist.mean','wg_dist.sd',\n",
    "            'sil', 'dbs', 'clusters', 'cluster_diff'\n",
    "       ]\n",
    "\n",
    "features_4bench = ['attr_conc.mean','attr_conc.sd','attr_ent.mean',\n",
    "    'attr_ent.sd','attr_to_inst','cohesiveness.mean','cohesiveness.sd',\n",
    "    #'cor.mean',#'cor.sd',\n",
    "    'cov.mean',#'cov.sd',\n",
    "    'eigenvalues.mean','eigenvalues.sd',\n",
    "    'inst_to_attr','iq_range.mean','iq_range.sd',\n",
    "    #'kurtosis.mean','kurtosis.sd',\n",
    "    'mad.mean','mad.sd',\n",
    "    #'max.mean','max.sd','mean.mean','mean.sd',\n",
    "    'median.mean',\n",
    "    'median.sd',#'min.mean','min.sd',\n",
    "    'nr_attr','nr_cor_attr','nr_inst','one_itemset.mean',\n",
    "    'one_itemset.sd',\n",
    "    #'range.mean','range.sd',\n",
    "    'sd.mean','sd.sd'\n",
    "    #,'skewness.mean', 'skewness.sd'\n",
    "    ,'sparsity.mean','sparsity.sd','t2','t3','t4','t_mean.mean',\n",
    "    't_mean.sd','two_itemset.mean','two_itemset.sd','var.mean','var.sd',\n",
    "    'wg_dist.mean','wg_dist.sd',\n",
    "    'sil', 'dbs', 'clusters', 'cluster_diff'\n",
    "]\n",
    "\n",
    "features = features_4bench\n",
    "\n",
    "\"\"\"## run optimization\"\"\"\n",
    "\n",
    "\n",
    "def run_exp(model_input, features, datasets_selected_benchmarking):\n",
    "\n",
    "  features_local = features[:-4]\n",
    "\n",
    "  #model = xgb.XGBRegressor()\n",
    "  #model.load_model(PATH+\"/xgb_Sv5.json\")\n",
    "\n",
    "  k_set = range(2,25,1)\n",
    "  #print(k_set)\n",
    "\n",
    "  mypath = PATH+\"validation_set_small\"\n",
    "  onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "  df_benchmark = pd.read_csv(PATH+\"metadatabase_validation_scaled.csv\")\n",
    "  df_benchmark = df_benchmark.iloc[::4,:]\n",
    "\n",
    "  #datasets_selected = ['iris.csv','a1.csv','tetra.csv','hepta.csv', 'target.csv', 'dim3.csv',\n",
    "  #                     'dim21-clusters10-instances2250-overlap1e-06-1e-05-aspectref3-aspectmaxmin3-radius5-imbalance1-rep1.csv',\n",
    "  #                     'dim25-clusters2-instances200-overlap1e-06-1e-05-aspectref1.5-aspectmaxmin1-radius1-imbalance1-rep1.csv',\n",
    "  #                     'dim6-clusters12-instances450-overlap1e-06-1e-05-aspectref1.5-aspectmaxmin3-radius1-imbalance1-rep1.csv']\n",
    "\n",
    "  #datasets_selected = ['dim15.csv','a1.csv','dim9.csv','dim12.csv','dim11.csv','wingnut.csv','unbalance.csv'\n",
    "#,'twodiamonds.csv','target.csv','tetra.csv','lsun3d.csv','iris.csv'\n",
    "#,'dim256.csv','dim128.csv','dim064.csv','dim032.csv','dim14.csv'\n",
    "#,'hepta.csv','dim13.csv','engytime.csv','dim10.csv','dim6.csv','dim5.csv'\n",
    "#,'dim3.csv','chainlink.csv','cancer.csv','dim4.csv','atom.csv'\n",
    "#,'dim7.csv','dim1024.csv','dim8.csv','dim512.csv']\n",
    "\n",
    "\n",
    "  datasets_selected = datasets_selected_benchmarking\n",
    "\n",
    "  # Extract all available unsupervised measures\n",
    "  print(\"Datasets: \",datasets_selected)\n",
    "  datasets = []\n",
    "  for file in datasets_selected:\n",
    "\n",
    "    #print(\">>>\"+file+\"<<<<\")\n",
    "    #try:\n",
    "    n_clusters_ideal = int(df_benchmark[df_benchmark[\"Dataset\"] == file].clusters)\n",
    "    #except:\n",
    "    #  print(\"ERROR**********************\")\n",
    "    #  continue\n",
    "    #print(\"n_clusters\", n_clusters_ideal)\n",
    "\n",
    "    X = pd.read_csv(mypath+\"/\"+file)\n",
    "    ft = df_benchmark[df_benchmark.Dataset == file].filter(features_local)\n",
    "    ft = ft[features_local].values\n",
    "\n",
    "    #print(X.isnull().any().head(40))\n",
    "\n",
    "    for rep in k_set:\n",
    "      cluster_algo = [AgglomerativeClustering(n_clusters=rep, metric='euclidean', linkage='ward'),\n",
    "                      KMeans(n_clusters=rep, n_init=\"auto\"),\n",
    "                      KMedoids(n_clusters=rep),\n",
    "                      MiniBatchKMeans(n_clusters=rep, batch_size=10,n_init=\"auto\")\n",
    "                      ]\n",
    "      sample_test = []\n",
    "      for c in cluster_algo:\n",
    "        cluster_labels = c.fit_predict(X)\n",
    "        sil = silhouette_score(X, cluster_labels)\n",
    "        dbs = davies_bouldin_score (X, cluster_labels)\n",
    "        mf = ft[0].tolist()\n",
    "        mf.extend([sil,dbs,rep])\n",
    "        yhat = model_input.predict([mf])\n",
    "        datasets.append([file]+[type(c).__name__]+[sil]+[dbs]+[rep]+[n_clusters_ideal]+[yhat])\n",
    "  return datasets\n",
    "\n",
    "\"\"\"# GridSearch\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "original = pd.read_csv(PATH+FILE)\n",
    "\n",
    "#combined_lists = [sil_ini_list, dbs_max_list, dist_sil_dbs_list, seed_list, distribution_list, algorithms]\n",
    "combined_lists = [sil_ini_list, dbs_max_list, dist_sil_dbs_list, seed_list, distribution_list, algorithms, contamination, qtd_arvores]\n",
    "\n",
    "all_combinations = list(itertools.product(*combined_lists))\n",
    "\n",
    "for index, combination in enumerate(all_combinations):\n",
    "  #run.update()\n",
    "  df_surrogate = original\n",
    "  progress = (index + 1) / len(all_combinations) * 100\n",
    "  print(\"\\n\\n\\n\")\n",
    "  print(f\"Progress: {progress:.2f}%\")\n",
    "  sil_ini, dbs_max, dist_s_d, seed, dist, regressor, contamin, qtd_arvores = combination\n",
    "\n",
    "  #df_surrogate = df_surrogate.sample(frac=1, replace=True, random_state=1).reset_index(drop=True)\n",
    "  #df_surrogate = df_surrogate.drop_duplicates(subset=[\"Dataset\"], keep=\"last\")\n",
    "\n",
    "  df_surrogate = filtering_distribution(df_surrogate, dist)\n",
    "  df_surrogate = filter_sil_bds(df_surrogate, sil_ini, dbs_max)\n",
    "  df_surrogate = filter_dist_sil_bds(df_surrogate, dist_s_d)\n",
    "\n",
    "  df_surrogate = filter_samples_isolation(df_surrogate, contamin)\n",
    "\n",
    "\n",
    "  if(df_surrogate.shape[0]<threshold_qtd_on_training):\n",
    "    print(\"Invalid Combination: \",df_surrogate.shape )\n",
    "    continue\n",
    "  print(\"Samples\", df_surrogate.shape)\n",
    "\n",
    "  df_surrogate['clusters'] = df_surrogate.apply(lambda row: int(row.Dataset.split('-')[1].replace(\"clusters\",\"\")), axis=1)\n",
    "\n",
    "  df_surrogate = df_surrogate[features]\n",
    "\n",
    "  data = df_surrogate\n",
    "  x_train, y_train = data.values[:, :-1], data.values[:, -1]\n",
    "\n",
    "  model_regressor = regressor(random_state=seed, n_estimators=qtd_arvores, n_jobs=-1)\n",
    "  model_regressor.fit(x_train, y_train)\n",
    "\n",
    "  run = wandb.init(project=\"Surrogate_Sv5\", entity=\"barbonjr\", reinit=True, name=\"A_\"+str(round(contamin,2))+\"_\"+str(seed)+\"_\"+str(round(sil_ini,2))+\"_\"+str(round(dbs_max,2))+\"_\"+str(round(dist_s_d,2)))\n",
    "\n",
    "  minimizing_logging(model_regressor, features, sil_ini, dbs_max, dist_s_d, seed, dist, df_surrogate.shape[0], run, progress, contamin, qtd_arvores)\n",
    "\n",
    "  ### WANDB\n",
    "  #run.log({\"Progress\": progress})\n",
    "  run.finish()\n",
    "  ### WANDB\n",
    "\n",
    "\"\"\"#Persistindo o Surrogate\"\"\"\n",
    "\n",
    "# export surrogate model\n",
    "#model.save_model(PATH+\"xgb_\"+VERSION+\".json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
